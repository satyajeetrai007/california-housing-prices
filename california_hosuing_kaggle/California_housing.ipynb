{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7876,"sourceType":"datasetVersion","datasetId":5227}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/california-housing-prices/housing.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport inspect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()\ndf.ocean_proximity.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_bedroom = df.total_bedrooms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(total_bedroom[total_bedroom.isna()])\n# replace total bedrooms NaN values with there median value\nmedian = total_bedroom.median()\nprint(f\"median is :{median}\")\ntotal_bedroom.fillna(total_bedroom.median(),inplace = True)\nprint(f\"the NAN values in total_bedroom now is :{total_bedroom.isna().sum()}\")\n# or total_bedroom.fillna(total_bedroom.median(),inplace = True) to do the same in one line \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf.plot()\n\n\n#  first i need to scale the  data , for that either i will use scikit-learn preprocessor or i can scale it by writing 4-5 line code ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nscaler = StandardScaler()\n\nnumeric_df = df.copy()\nnumeric_df.drop(columns =['ocean_proximity'],inplace = True)\n\n# scaled_numeric_data = scaler.fit_transform(numeric_df)\n# SINCE THE VALUES FOR HOUSE VALUE WAS COMING NEGATIVE BY USING STANDARD SCALAR METHOD I WILL USE NORMALIZATION \n\n\nscaled_numeric_data = scaler.fit_transform(numeric_df)\n\n\nscaled_numeric_data = pd.DataFrame(scaled_numeric_data,columns = numeric_df.columns)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix  = scaled_numeric_data.corr()\nplt.figure(figsize=(10,10))\n# plt.imshow(corr_matrix, cmap ='coolwarm',interpolation='nearest') ----- i donot like this plot let's try with sns \n\nsns.heatmap(corr_matrix , annot=True , cmap= 'coolwarm')\nplt.title(\"Heat_Map_Of_Correlation_Matrix\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numeric_dataframe(dataframe):\n    numeric_df = dataframe.select_dtypes(include ='number')\n    return numeric_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_scatter_matrix(numeric_dataframe):\n  \n    # if i just want to plot some of the attributes then i will make a list of them and will pass it to dataframe in the scatter matrix function \n    # attributes = ['A','B']\n    # scatter_matrix(dataframe[attributes],figsize = (10,8))\n    # i will plot the correcln matrix for all the attributes present in the DataFrame \n    scatter_matrix(numeric_dataframe,figsize=(10,15),hist_kwds={'bins': 20},alpha =0.5,diagonal='kde')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_df = numeric_dataframe(df)\n# plot_scatter_matrix(numeric_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now for non-numeric columns i need  one hot encoding \nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse_output = False)\n\nocean_proximity  = df[['ocean_proximity']]\n# non_numeric = encoder.fit_transform(ocean_proximity)\n# non_numeric is a sparse matrix (which stores the location of category =1 , it is efficent way of utilixing \n#memory when dataset is very large , by default sparse = true ,if we do sparse = False then this(variable) will be a dense matrix )\n# use OneHotEncoder (sparse_output = False) to get a dense matrix \nnon_numeric = encoder.fit_transform(ocean_proximity)\nencoding_columns = encoder.get_feature_names_out(['ocean_proximity'])# to get the names of column , we need it when we make DataFrame and concatenate the value with the numeric values \nnon_numeric = pd.DataFrame(non_numeric,columns = ['less_than1H_ocean','INLAND','ISLAND','NEAR_BAY','NEAR_OCEAN'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = pd.concat([non_numeric,scaled_numeric_data],axis =1)\nnew_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = new_df['median_house_value']\nprint(new_df.columns)\nX = new_df.drop(columns = ['median_house_value'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.hist(bins =100,figsize=(20,20));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_train , x_test , y_train , y_test = train_test_split(X,Y,train_size= 0.7,random_state=13)\n# y_train = y_train.values.reshape(-1,1)# to reshape in 2 dimensional array which have 1 columns \n# y_test = y_test.values.reshape(-1,1)\n# randomForestClassifer asking for (n,) 1 dimensional array \nx_train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test,x_dev,y_test,y_dev = train_test_split(x_test,y_test,test_size = 0.5)\nprint(x_test.shape,x_dev.shape,y_test.shape ,y_dev.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef model_check_cv(X,Y,Model):\n    \n    x_train,x_test,y_train, y_test = train_test_split(X,Y,test_size =0.3)\n    x_test,x_dev,y_test,y_dev= train_test_split(x_test,y_test,test_size = 0.3)\n    print(f\" shapes are x_train -{x_train.shape} , y_train-   {y_train.shape}, x_test - {x_test.shape} ,y_test-  {y_test.shape} , x_dev -  {x_dev.shape}   , y_dev -{y_dev.shape}\")\n\n    model = Model\n   \n    scores = cross_val_score(Model,x_train,y_train,scoring = 'neg_mean_squared_error',cv=10)\n    rmse_score = np.sqrt(-scores)\n\n    print(\"scores : \",rmse_score)\n    print(\"mean : \",rmse_score.mean())\n    print(\"standard_deviation : \",rmse_score.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nmodel_check_cv(X,Y,XGBRegressor())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_check_cv(X,Y,RandomForestRegressor())\nprint()\nprint()\nmodel_check_cv(X,Y,LinearRegression())\nprint()\nprint()\nmodel_check_cv(X,Y,DecisionTreeRegressor())\nprint()\nprint()\nmodel_check_cv(X,Y,SVR())\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid search for Random Forest\nrandom_forest = RandomForestRegressor()\nparameter_grid =[{'n_estimators':[330,350,370,390],\n                  'max_depth':[29]}]\ngrid_search = GridSearchCV(random_forest,parameter_grid,cv=5,scoring = 'neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(x_dev,y_dev)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid serach for Decision Trees\nDecisionTreeRegressor()\nparameter_grid =[{'max_depth':[10,20,30],\n                  'min_samples_leaf':[1,2,10,20,30,40,50,100,800],\n                 'max_features':[1,2,5,10,20,30,50,40],\n                }]\ngrid_search = GridSearchCV(DecisionTreeRegressor(),parameter_grid,cv=5,scoring = 'neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(x_dev,y_dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xg boost regressor hyperparameter \nxg_regressor = XGBRegressor()\nparameter_grid =[{'learning_rate':[0.0001,0.00001],\n                  'max_depth':[29]}]\ngrid_search = GridSearchCV(random_forest,parameter_grid,cv=5,scoring = 'neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(x_dev,y_dev)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################### i havenot studied SVM so i donot understand its hyperparameters\n#grid search for SVM Regressor\n# parameter_grid =[{'max_de':[330,350,370,390],\n#                   'max_depth':[29]}]\n# grid_search = GridSearchCV(SVR(),parameter_grid,cv=5,scoring = 'neg_mean_squared_error',return_train_score=True)\n# grid_search.fit(x_dev,y_dev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use this to check the arguments which we can pass for a function\n\ninspect.signature(sns.heatmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_check(X,Y,LinearRegression(copy_X=True))\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}